{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description**: demonstrates a significant speedup over the standard pipeline on dummy\n",
    "prompts.\n",
    "\n",
    "**Estimated run time**: ~1 min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from backprompt import Text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A prompt string can usually be split up into two parts: `context + request`. The\n",
    "`context` is assumed to be fixed across all prompts. The `request` is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = '''\n",
    "This is a relatively long string. It's used (exactly as is) to provide context for many\n",
    "future model calls. We'll cache the model's representation of it to save inference time\n",
    "for those calls.\n",
    "\n",
    "First, a description of the task: description of task\n",
    "\n",
    "Here's a list of choices, exemplars, etc.\n",
    "\n",
    "Thing 1: description for thing 1\n",
    "\n",
    "Thing 2: description for thing 2\n",
    "\n",
    "Thing 3: description for thing 3\n",
    "\n",
    "Thing 4: description for thing 4\n",
    "\n",
    "Thing 5: description for thing 5\n",
    "\n",
    "Thing 6: description for thing 6\n",
    "\n",
    "Thing 7: description for thing 7\n",
    "\n",
    "Thing 8: description for thing 8\n",
    "\n",
    "Thing 9: description for thing 9\n",
    "\n",
    "Thing 10: description for thing 10\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The request is variable. It depends on a new piece of text, and requests that the LLM\n",
    "does something with it using the context in `context`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_template = '''\n",
    "Here's my new text: {text}\n",
    "\n",
    "Do something with it.\n",
    "'''\n",
    "\n",
    "num_requests = 20\n",
    "requests = [request_template.format(text=i) for i in range(num_requests)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model_name = 'gpt2'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_and_tokenizer = (model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this object should be static\n",
    "context_cached = Text(context, model_and_tokenizer)\n",
    "# optional: cache the model's representation of it now\n",
    "context_cached()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`backprompt` doesn't support batching yet. Let's mimic live inference, which usually consists of 1-by-1 / non-batched model calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87681f39290f471bbdd246aca8da94de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 6.2 s\n",
      "Wall time: 4.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for request in tqdm(requests):\n",
    "    # wrap it in a Text\n",
    "    request = Text(request, model_and_tokenizer)\n",
    "    # construct the prompt / concatenate them by adding the Text objs\n",
    "    prompt = context_cached + request\n",
    "    # get next-token logits for all tokens in prompt_request\n",
    "    prompt()\n",
    "    _ = prompt.model_repr[1].logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this to the plain pipeline (on CPU):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "909c16d7199c414d94f54f835e90da5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 37 s\n",
      "Wall time: 20.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for request in tqdm(requests):\n",
    "    # construct the prompt by concatenating the Text objs\n",
    "    prompt = context + request\n",
    "    # tokenize\n",
    "    encoding = tokenizer([prompt], return_tensors=\"pt\", padding=True).to(model.device) \n",
    "    # get next-token logits for all tokens in prompt\n",
    "    with torch.no_grad():\n",
    "        _ = model(**encoding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much overhead was needed to achieve this speedup?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 MB\n"
     ]
    }
   ],
   "source": [
    "cached_encoding, cached_out = context_cached.model_repr\n",
    "\n",
    "def memory_mb(tensor: torch.Tensor) -> float:\n",
    "    return tensor.untyped_storage().size() / 1e6\n",
    "\n",
    "cached_encoding_memory_mb = sum(\n",
    "    memory_mb(tensor) for tensor in cached_encoding.values()\n",
    ")\n",
    "\n",
    "cached_out_memory_mb = memory_mb(cached_out.logits)\n",
    "cached_out_memory_mb += memory_mb(\n",
    "    torch.stack([torch.stack(block) for block in cached_out.past_key_values], dim=0)\n",
    ")\n",
    "\n",
    "_cache_memory_mb = cached_encoding_memory_mb + cached_out_memory_mb\n",
    "print(f'{round(_cache_memory_mb)} MB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backprompt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
